# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18dPqGXpqIPBx8IigkLHU9iwYFktFU2i2
"""



import numpy as np
import sklearn.decomposition
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models

NUM_EPOCHS = 10
N = 2 #N = count of mixed videos

lr1 = 1e-4
lr2 = 1e-3
lr3 = 1e-3

opt_V = optim.SGD(V.parameters(), lr=lr1)
opt_U = optim.SGD(U.parameters(), lr=lr2)
opt_G = optim.SGD(G.parameters(), lr=lr3)

V = Video()
U = Unet()
G = Generator()

criterion = nn.BCELoss()

print_loss_freq = 50 #print loss every print_loss_freq batches.

save_freq = 300

example_freq = 400

batch_count = len(dataloader)

PATH_U = 
PATH_V =
PATH_G = 
PATH_EPOCH = 
from_save = 0
start_epoch = 0
ngpu = 1
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

if (from_save == 1):
    U = torch.load(PATH_U)
    V = torch.load(PATH_V)
    G = torch.load(PATH_G)
    start_epoch = torch.load(PATH_EPOCH)

if (device.type == 'cuda') and (ngpu > 1):
    U = nn.DataParallel(U, list(range(ngpu)))
    V = nn.DataParallel(V, list(range(ngpu)))
    G = nn.DataParallel(G, list(range(ngpu)))

for epoch in range(start_epoch, NUM_EPOCHS):
    for batch_n, data in enumerate(dataloader, 0):
        audio_sum = torch.sum(data[1], 1).to(device)
        losses = []
        for i in range(N):
            U.zero_grad()
            V.zero_grad()
            G.zero_grad()
            
            video = data[0][:, i].to(device)
            
            U_res = U(audio_sum)
            V_res = V(video)
            G_res = G(V_res, U_res)#(bs, x, y, t, freq)
            model_answer = torch.mul(G_res, audio_sum[None, 1, 1, None, None])#(bs, x, y, t, freq) * (bs, t, freq)

            loss = criterion(torch.sum(model_answer, [1, 2]), data[1][:, i, :] / audio_sum)
            losses.append(loss)
            loss.backward()

            opt_V.step()
            opt_U.step()
            opt_G.step()


        if (batch_n % print_loss_freq == 0):
            test_loss = []
            with torch.no_grad():
                for batch_n, data in enumerate(data_test_loader, 0):
                    audio_sum = torch.sum(data[1], 1).to(device)
                    for i in range(N):
                        video = data[0][:, i].to(device)

                        U_res = U(audio_sum)
                        V_res = V(video)
                        G_res = G(V_res, U_res)#(bs, x, y, t, freq)
                        model_answer = torch.mul(G_res, audio_sum[None, 1, 1, None, None])#(bs, x, y, t, freq) * (bs, t, freq)

                        loss = criterion(torch.sum(model_answer, [1, 2]), data[1][:, i, :] / audio_sum)
                        test_loss.append(loss)
            


            print('epoch [%d/%d]\t batch [%d/%d]\t. Train loss: %d,\t test loss: %d' % (epoch, NUM_EPOCHS, batch_n, batch_count, losses.mean()), test_loss.mean())

        if (batch_n % save_freq == 0):
            torch.save(U, PATH_U)
            torch.save(V, PATH_V)
            torch.save(G, PATH_G)
            torch.save(epoch, PATH_EPOCH)


        if (batch_n % example_freq == 0):
            with torch.no_grad():
                picture = data[0][-1, 0, :, :, :, -1]
                video = data[0][-1:, 0, :, :, :, :]
                audio = data[1][-1:, 0, :]
                U_sample_res = U(audio)
                V_sample_res = V(video)
                G_sample_res = G(V_sample_res, U_sample_res)
                model_sample_answer = torch.mul(G_sample_res, audio[None, 1, 1, None, None])
                pca = sklearn.decomposition.PCA(n_components=3)
                vectors_square = model_sample_answer[-1, :, :, -1, :]
                vectors_flatten = vectors_square.reshape(-1, vectors_square.shape()[-1]).numpy()
                rgb = pca.fit_transform(vectors_flatten)
                rgb_picture = torch.from_numpy(rgb).reshape(vecrors_square.shape(0)[:2] + (-1,))
                located_sound_picture = torch.permute(rgb_picture, [2, 0, 1])
                output = torch.permute(located_sound_picture * 0.2 + picture * 0.8)
                fig = plt.figure(figsize=(8,8))
                plt.axis("off")
                plt.imshow(np.transpose(output.numpy(), (1, 2, 0)))







